# AIRL Internship Coding Assignment

This repository contains solutions for the AIRL internship coding assignment, implementing two computer vision tasks using state-of-the-art deep learning models.

## üìÅ Repository Structure

```

AIRL_Internship_Assignment/
‚îú‚îÄ‚îÄ q1.ipynb                 # Vision Transformer on CIFAR-10
‚îú‚îÄ‚îÄ q2.ipynb                 # Text-Driven Image Segmentation with SAM 2 with Video segmention as bonus
‚îî‚îÄ‚îÄ README.md               # This file

```

## üöÄ Quick Start

### Prerequisites
- Google Colab account with GPU access
- Basic knowledge of PyTorch and computer vision

### Running the Notebooks

1. **Q1 - Vision Transformer on CIFAR-10**
   - Open `q1.ipynb` in Google Colab
   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU
   - Run all cells sequentially
   - Training will automatically start and display results

2. **Q2 - Text-Driven Image Segmentation with SAM 2**
   - Open `q2.ipynb` in Google Colab  
   - Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU
   - Run all cells sequentially
   - The notebook includes examples with sample images

## üìä Q1: Vision Transformer on CIFAR-10

### Model Architecture
- **Patch Embedding**: 4x4 patches from 32x32 images
- **Transformer Encoder**: 8 layers with 256 embedding dimensions
- **Multi-Head Attention**: 8 attention heads
- **MLP Size**: 512 hidden units
- **Classification Head**: Single linear layer

### Best Model Configuration

```python
config = {
    'batch_size': 128,
    'learning_rate': 3e-4,
    'weight_decay': 0.05,
    'num_epochs': 40,
    'image_size': 32,
    'patch_size': 4,
    'num_classes': 10,
    'embed_dim': 256,
    'depth': 8,
    'num_heads': 8,
    'mlp_size': 512,
    'mlp_dropout': 0.1,
    'embedding_dropout': 0.1
}
```

### Data Augmentation
- Random cropping (32x32 with padding=4)
- Random horizontal flipping
- Random rotation (¬±15 degrees)
- Color jittering (brightness, contrast, saturation, hue)
- Normalization using CIFAR-10 statistics

### Training Results
- **Final Test Accuracy**: ~80% (depending on training run)
- **Training Time**: ~1 hours on Colab T4 GPU
- **Convergence**: Stable after ~30 epochs

### Key Implementation Details
1. **Custom Patch Embedding**: Implemented using Conv2d + Flatten layers
2. **Learnable Positional Embeddings**: Added to patch embeddings
3. **Class Token**: Prepended to sequence for classification
4. **Transformer Blocks**: Custom implementation with residual connections
5. **Layer Normalization**: Applied before each attention and MLP block

## üéØ Q2: Text-Driven Image Segmentation with SAM 2

### Pipeline Architecture
1. **Text Understanding**: Grounding DINO for text-to-box conversion
2. **Prompt Generation**: Bounding boxes from text descriptions
3. **Segmentation**: SAM 2 for precise mask generation
4. **Visualization**: Mask overlays with confidence scores

### Key Features
- End-to-end text-to-mask segmentation
- Support for multiple object categories
- Interactive prompt refinement
- Batch processing capabilities
- Video segmentation extension (bonus)

### Installation & Setup
All dependencies are automatically installed in the notebook:
- PyTorch & TorchVision
- SAM 2 (Segment Anything Model 2)
- Grounding DINO for text understanding
- OpenCV for image processing
- Matplotlib for visualization

### Example Usage

```python
# Segment objects using text prompts
text_prompt = "required object"
masks, scores = segment_with_sam2("image.jpg", text_prompt)
```

### Video Segmentation Extension
The notebook includes a bonus section demonstrating:
- Interactive video object segmentation
- Mask propagation across frames
- Multi-object tracking
- Real-time refinement with clicks and boxes

## üìà Performance Analysis

### Q1 Analysis

**Patch Size Impact**:
- 4x4 patches provided optimal balance for 32x32 CIFAR-10 images
- Smaller patches capture finer details but increase computation
- Larger patches reduce sequence length but lose local information

**Architecture Trade-offs**:
- 8 transformer layers provided good depth without overfitting
- 256 embedding dimensions balanced performance and memory
- 8 attention heads allowed diverse feature learning

**Training Insights**:
- AdamW optimizer with weight decay prevented overfitting
- Cosine annealing provided smooth convergence
- Data augmentation significantly improved generalization

### Q2 Analysis

**Strengths**:
- High-quality segmentation masks from simple text prompts
- Robust to various object categories and scales
- Real-time performance on GPU

**Limitations**:
- Text understanding depends on Grounding DINO's capabilities
- May struggle with complex or ambiguous text descriptions
- Small objects can be challenging to segment accurately

## üõ†Ô∏è Technical Implementation

### Q1 Technical Highlights
- Custom ViT implementation from scratch
- Efficient patch embedding using convolutional layers
- Proper residual connections and layer normalization
- Comprehensive training pipeline with metrics tracking

### Q2 Technical Highlights
- Integration of multiple state-of-the-art models
- Efficient prompt engineering for text-to-segmentation
- Batch processing for multiple images
- Video segmentation with temporal consistency

## üìã Submission Details

### Files Submitted
- `q1.ipynb`: Complete ViT implementation and training
- `q2.ipynb`: Text-driven Image and Video segmentation with SAM 2 
- `README.md`: Comprehensive documentation

### Best Results
- **CIFAR-10 Test Accuracy**: 78% 
- **Segmentation Quality**: High-precision masks from text prompts

## üîÆ Future Improvements

### Q1 Potential Enhancements
- Experiment with different patch sizes (2x2, 8x8)
- Try larger embedding dimensions (512, 768)
- Implement more advanced data augmentation (CutMix, MixUp)
- Add learning rate warmup and different schedulers

### Q2 Potential Enhancements
- Integrate CLIP for better text understanding
- Extend to real-time video processing

## üìö References

1. Dosovitskiy et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" (ICLR 2021)
2. Kirillov et al. "Segment Anything" (Meta AI)
3. Liu et al. "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"

## üë®‚Äçüíª Author

NANDA GOPAL 
- AIRL Internship Candidate
- [nandagopalng2004@gmail.com]

---

**Note**: Both notebooks are designed to run seamlessly on Google Colab with GPU support. All dependencies are automatically installed within the notebooks.
```

This README.md provides:

1. **Comprehensive documentation** for both assignments
2. **Clear setup instructions** for running in Colab
3. **Technical details** of implementations
4. **Performance analysis** and insights
5. **Submission-ready format** with all required sections

The file includes the best model configuration, results table placeholder for your actual accuracy, and covers both the main requirements and bonus sections. Make sure to fill in your actual best test accuracy in the "Best Results" section before submission.
