{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Getting setup\n",
        "\n",
        "As we've done previously, let's make sure we've got all of the modules we'll need for this section.\n",
        "\n",
        "We'll import the Python scripts (such as `data_setup.py` and `engine.py`) we created in [ PyTorch Going Modular](https://github.com/NANDAGOPALNG/pytorch--going_modular).\n",
        "\n",
        "To do so, we'll download [`going_modular`](https://github.com/NANDAGOPALNG/pytorch--going_modular/going_modular) directory from the `pytorch-deep-learning` repository (if we don't already have it).\n",
        "\n",
        "We'll also get the [`torchinfo`](https://github.com/TylerYep/torchinfo) package if it's not available.\n",
        "\n",
        "`torchinfo` will help later on to give us a visual representation of our model.\n",
        "\n",
        "And since later on we'll be using `torchvision`"
      ],
      "metadata": {
        "id": "osuF5XRIV78s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jPqTwZsdJIJ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/NANDAGOPALNG/pytorch--going_modular\n",
        "    !mv pytorch--going_modular/going_modular .\n",
        "    !mv pytorch--going_modular/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch--going_modular\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "id": "2JcOaJ4nkbJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "oFj178kfc2na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "LxgBVOiMdQ8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have downloaded the dataset from -- https://www.cs.toronto.edu/%7Ekriz/cifar.html"
      ],
      "metadata": {
        "id": "SEZkMnpCd2Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Enhanced data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "metadata": {
        "id": "1R4e_XWucn3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dir = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
      ],
      "metadata": {
        "id": "sjnv0gGJcrOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 128 # this is lower than the ViT paper but it's because we're starting small\n",
        "IMG_SIZE = 32\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = DataLoader(train_dir,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_dir,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)\n",
        "\n",
        "class_names = train_dir.classes\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "g8TH9t7Rp2lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# Get a single image from the batch\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the batch shapes\n",
        "image.shape, label"
      ],
      "metadata": {
        "id": "uPd3yIJGp6eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot image with matplotlib\n",
        "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "3nqBQDLNqCiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replicating the ViT paper"
      ],
      "metadata": {
        "id": "Apv4dCkbqg03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by going through Figure 1 of the ViT Paper.\n",
        "\n",
        "The main things we'll be paying attention to are:\n",
        "1. **Layers** - takes an **input**, performs an operation or function on the input, produces an **output**.\n",
        "2. **Blocks** - a collection of layers, which in turn also takes an **input** and produces an **output**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs.png\" width=900 alt=\"figure 1 from the original vision transformer paper\"/>\n",
        "\n",
        "*Figure 1 from the ViT Paper showcasing the different inputs, outputs, layers and blocks that create the architecture. Our goal will be to replicate each of these using PyTorch code.*\n",
        "\n",
        "The ViT architecture is comprised of several stages:\n",
        "* **Patch + Position Embedding (inputs)** - Turns the input image into a sequence of image patches and adds a position number to specify in what order the patch comes in.\n",
        "* **Linear projection of flattened patches (Embedded Patches)** - The image patches get turned into an **embedding**, the benefit of using an embedding rather than just the image values is that an embedding is a *learnable* representation (typically in the form of a vector) of the image that can improve with training.\n",
        "* **Norm** - This is short for \"[Layer Normalization](https://paperswithcode.com/method/layer-normalization)\" or \"LayerNorm\", a technique for regularizing (reducing overfitting) a neural network, you can use LayerNorm via the PyTorch layer [`torch.nn.LayerNorm()`](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
        "* **Multi-Head Attention** - This is a [Multi-Headed Self-Attention layer](https://paperswithcode.com/method/multi-head-attention) or \"MSA\" for short. You can create an MSA layer via the PyTorch layer [`torch.nn.MultiheadAttention()`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html).\n",
        "* **MLP (or [Multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron))** - A MLP can often refer to any collection of feedforward layers (or in PyTorch's case, a collection of layers with a `forward()` method). In the ViT Paper, the authors refer to the MLP as \"MLP block\" and it contains two [`torch.nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers with a [`torch.nn.GELU()`](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) non-linearity activation in between them (section 3.1) and a [`torch.nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) layer after each (Appendix B.1).\n",
        "* **Transformer Encoder** - The Transformer Encoder, is a collection of the layers listed above. There are two skip connections inside the Transformer encoder (the \"+\" symbols) meaning the layer's inputs are fed directly to immediate layers as well as subsequent layers. The overall ViT architecture is comprised of a number of Transformer encoders stacked on top of eachother.\n",
        "* **MLP Head** - This is the output layer of the architecture, it converts the learned features of an input to a class output. Since we're working on image classification, you could also call this the \"classifier head\". The structure of the MLP Head is similar to the MLP block."
      ],
      "metadata": {
        "id": "plWMoBfQe29H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Exploring the Four Equations\n",
        "\n",
        "The next main part(s) of the ViT paper we're going to look at are the four equations in section 3.1.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png\" width=650 alt=\"four mathematical equations from the vision transformer machine learning paper\"/>\n",
        "\n",
        "*These four equations represent the math behind the four major parts of the ViT architecture.*\n",
        "\n",
        "\n",
        "\n",
        "| **Equation number** | **Description from ViT paper section 3.1** |\n",
        "| ----- | ----- |\n",
        "| 1 | ...The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a **trainable linear projection** (Eq. 1). We refer to the output of this projection as the **patch embeddings**... **Position embeddings** are added to the patch embeddings to retain positional information. We use standard **learnable 1D position embeddings**...|\n",
        "| 2 | The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). **Layernorm (LN) is applied before every block**, and **residual connections after every block** (Wang et al., 2019; Baevski & Auli, 2019). |\n",
        "| 3 | Same as equation 2. |\n",
        "| 4 | Similar to BERT's [ class ] token, we **prepend a learnable embedding to the sequence of embedded patches** $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(\\mathbf{z}_{L}^{0}\\right)$ serves as the image representation $\\mathbf{y}$ (Eq. 4)... |\n",
        "\n",
        "\n",
        "*Connecting Figure 1 from the ViT paper to the four equations from section 3.1 describing the math behind each of the layers/blocks.*\n",
        "\n",
        "There's a lot happening in the image above but following the coloured lines and arrows reveals the main concepts of the ViT architecture.\n",
        "\n",
        "How about we break down each equation further (it will be our goal to recreate these with code)?\n",
        "\n",
        "In all equations (except equation 4), \"$\\mathbf{z}$\" is the raw output of a particular layer:\n",
        "\n",
        "1. $\\mathbf{z}_{0}$ is \"z zero\" (this is the output of the initial patch embedding layer).\n",
        "2. $\\mathbf{z}_{\\ell}^{\\prime}$ is \"z of a particular layer *prime*\" (or an intermediary value of z).\n",
        "3. $\\mathbf{z}_{\\ell}$ is \"z of a particular layer\".\n",
        "\n",
        "And $\\mathbf{y}$ is the overall output of the architecture."
      ],
      "metadata": {
        "id": "Zu_DyjBCfTSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating patch embedding input and output shapes by hand"
      ],
      "metadata": {
        "id": "-lvtltRsrfw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create example values\n",
        "height = 32 # H\n",
        "width = 32 # W\n",
        "color_channels = 3 # C\n",
        "patch_size = 4 # P\n",
        "\n",
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"
      ],
      "metadata": {
        "id": "1DgdLud-qGEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape (this is the size of a single image)\n",
        "embedding_layer_input_shape = (height, width, color_channels)\n",
        "\n",
        "# Output shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
        "\n",
        "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
      ],
      "metadata": {
        "id": "7D-F0ToZrew5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turning a single image into patches"
      ],
      "metadata": {
        "id": "4hJ1GITKs6es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "fCmRBMPkrmvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
        "image_permuted = image.permute(1, 2, 0)\n",
        "\n",
        "# Index to plot the top row of patched pixels\n",
        "patch_size = 4\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :]);"
      ],
      "metadata": {
        "id": "AgJCH090s9ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 32\n",
        "patch_size = 4\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=1,\n",
        "                        ncols=img_size // patch_size, # one column for each patch\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Iterate through number of patches in the top row\n",
        "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
        "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n",
        "    axs[i].set_xlabel(i+1) # set the label\n",
        "    axs[i].set_xticks([])\n",
        "    axs[i].set_yticks([])"
      ],
      "metadata": {
        "id": "UNGc4q8mta1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
        "img_size = 32\n",
        "patch_size = 4\n",
        "num_patches = img_size/patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\\n",
        "        \\nNumber of patches per column: {num_patches}\\\n",
        "        \\nTotal patches: {num_patches*num_patches}\\\n",
        "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n",
        "                        ncols=img_size // patch_size,\n",
        "                        figsize=(num_patches, num_patches),\n",
        "                        sharex=True,\n",
        "                        sharey=True)\n",
        "\n",
        "# Loop through height and width of image\n",
        "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
        "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
        "\n",
        "        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
        "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n",
        "                                        patch_width:patch_width+patch_size, # iterate through width\n",
        "                                        :]) # get all color channels\n",
        "\n",
        "        # Set up label information, remove the ticks for clarity and set labels to outside\n",
        "        axs[i, j].set_ylabel(i+1,\n",
        "                             rotation=\"horizontal\",\n",
        "                             horizontalalignment=\"right\",\n",
        "                             verticalalignment=\"center\")\n",
        "        axs[i, j].set_xlabel(j+1)\n",
        "        axs[i, j].set_xticks([])\n",
        "        axs[i, j].set_yticks([])\n",
        "        axs[i, j].label_outer()\n",
        "\n",
        "# Set a super title\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xgjBD3STtigw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating image patches with torch.nn.Conv2d()"
      ],
      "metadata": {
        "id": "MmHdAkmTt4A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size=4\n",
        "\n",
        "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3, # number of color channels\n",
        "                   out_channels=256,\n",
        "                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
        "                   stride=patch_size,\n",
        "                   padding=0)"
      ],
      "metadata": {
        "id": "bNAowOyztoh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "0_pityv3uAh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the image through the convolutional layer\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
        "print(image_out_of_conv.shape)"
      ],
      "metadata": {
        "id": "Tj4VgxtluDMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.Size([1, 256, 8, 8]) -> [batch_size, embedding_dim, feature_map_height, feature_map_width]"
      ],
      "metadata": {
        "id": "GInG1VdDuOS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot random 5 convolutional feature maps\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 256), k=5) # pick 5 numbers between 0 and the embedding size\n",
        "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
        "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
        "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
      ],
      "metadata": {
        "id": "1kD74kJouGQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
        "single_feature_map, single_feature_map.requires_grad"
      ],
      "metadata": {
        "id": "NOd1BlAOuUVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening the patch embedding with torch.nn.Flatten()"
      ],
      "metadata": {
        "id": "z1oOr5pjukp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current tensor shape\n",
        "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
      ],
      "metadata": {
        "id": "5pzWHyEduhNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create flatten layer\n",
        "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
        "                     end_dim=3) # flatten feature_map_width (dimension 3)"
      ],
      "metadata": {
        "id": "p0WLvzzauv3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. View single image\n",
        "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "# 2. Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
        "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# 3. Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
        "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
      ],
      "metadata": {
        "id": "lbEMDgLuuzzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get flattened image patch embeddings in right shape\n",
        "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
        "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
      ],
      "metadata": {
        "id": "LiOYYH3Ru4Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single flattened feature map\n",
        "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
        "\n",
        "# Plot the flattened feature map visually\n",
        "plt.figure(figsize=(22, 22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "id": "CGi3bw7Ku-AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the flattened feature map as a tensor\n",
        "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
      ],
      "metadata": {
        "id": "9-Nx2ZhMvBK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turning the ViT patch embedding layer into a PyTorch module"
      ],
      "metadata": {
        "id": "UeeAF5LlvNEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class which subclasses nn.Module\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with appropriate variables\n",
        "    def __init__(self,\n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=4,\n",
        "                 embedding_dim:int=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create a layer to turn an image into patches\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "                                  end_dim=3)\n",
        "\n",
        "    # 5. Define the forward method\n",
        "    def forward(self, x):\n",
        "        # Create assertion to check that inputs are the correct shape\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
        "\n",
        "        # Perform the forward pass\n",
        "        x_patched = self.patcher(x)\n",
        "        x_flattened = self.flatten(x_patched)\n",
        "        # 6. Make sure the output shape has the right order\n",
        "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ],
      "metadata": {
        "id": "uuxZhreLvFzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create an instance of patch embedding layer\n",
        "patchify = PatchEmbedding(in_channels=3,\n",
        "                          patch_size=4,\n",
        "                          embedding_dim=256)\n",
        "\n",
        "# Pass a single image through\n",
        "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
        "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
        "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
      ],
      "metadata": {
        "id": "qE39vRsdvRdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the patch embedding and patch embedding shape\n",
        "print(patch_embedded_image)\n",
        "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "hzFY1UVsvdwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dimension = patch_embedded_image.shape[-1]\n",
        "\n",
        "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
        "                           requires_grad=True) # make sure the embedding is learnable\n",
        "\n",
        "# Show the first 10 examples of the class_token\n",
        "print(class_token[:, :, :10])\n",
        "\n",
        "# Print the class_token shape\n",
        "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "P3aFhcYwvhLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the class token embedding to the front of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
        "                                                      dim=1) # concat on first dimension\n",
        "\n",
        "# Print the sequence of patch embeddings with the prepended class token embedding\n",
        "print(patch_embedded_image_with_class_embedding)\n",
        "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "5iHwgZvjvl2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the position embedding"
      ],
      "metadata": {
        "id": "HWtf6K0ywSZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the sequence of patch embeddings with the prepended class embedding\n",
        "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
      ],
      "metadata": {
        "id": "ITM75VNovzT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate N (number of patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(1,\n",
        "                                             number_of_patches+1,\n",
        "                                             embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
        "print(position_embedding[:, :10, :10])\n",
        "print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "heeJ7pVGwflc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the position embedding to the patch and class token embedding\n",
        "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
        "print(patch_and_position_embedding)\n",
        "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
      ],
      "metadata": {
        "id": "dM4zZUxGwnyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together: from image to embedding"
      ],
      "metadata": {
        "id": "zF8cCbm7w-Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# 1. Set patch size\n",
        "patch_size = 4\n",
        "\n",
        "# 2. Print shape of original image tensor and get the image dimensions\n",
        "print(f\"Image tensor shape: {image.shape}\")\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "\n",
        "# 3. Get image tensor and add batch dimension\n",
        "x = image.unsqueeze(0)\n",
        "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
        "\n",
        "# 4. Create patch embedding layer\n",
        "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
        "                                       patch_size=patch_size,\n",
        "                                       embedding_dim=256)\n",
        "\n",
        "# 5. Pass image through patch embedding layer\n",
        "patch_embedding = patch_embedding_layer(x)\n",
        "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
        "\n",
        "# 6. Create class token embedding\n",
        "batch_size = patch_embedding.shape[0]\n",
        "embedding_dimension = patch_embedding.shape[-1]\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
        "                           requires_grad=True) # make sure it's learnable\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "\n",
        "# 7. Prepend class token embedding to patch embedding\n",
        "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
        "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
        "\n",
        "# 8. Create position embedding\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
        "                                  requires_grad=True) # make sure it's learnable\n",
        "\n",
        "# 9. Add position embedding to patch embedding with class token\n",
        "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
        "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
      ],
      "metadata": {
        "id": "bTmMqYXGw1oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation 2: Multi-Head Attention (MSA)"
      ],
      "metadata": {
        "id": "nTfl_2DG1DpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LayerNorm (LN) layer"
      ],
      "metadata": {
        "id": "7bmqbgNA1IvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Multi-Head Self Attention (MSA) layer"
      ],
      "metadata": {
        "id": "72YR6tYU1ML5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replicating Equation 2 with PyTorch layers"
      ],
      "metadata": {
        "id": "PuFtYKfo1P90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0.1): # doesn't look like the paper uses any dropout in MSABlocks\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multi-Head Attention (MSA) layer\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True) # does our batch dimension come first?\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n",
        "                                             key=x, # key embeddings\n",
        "                                             value=x, # value embeddings\n",
        "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
        "        return attn_output"
      ],
      "metadata": {
        "id": "u6eZCJYRzPHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MSABlock\n",
        "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=256, # from Table 1\n",
        "                                                             num_heads=8) # from Table 1\n",
        "\n",
        "# Pass patch and position image embedding through MSABlock\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
        "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
        "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
      ],
      "metadata": {
        "id": "xhKU_zH22PFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation 3: Multilayer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "T-aI825s2lYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replicating Equation 3 with PyTorch layers"
      ],
      "metadata": {
        "id": "if1sTjvt2sch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=256, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=512, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
        "                      out_features=embedding_dim), # take back to embedding_dim\n",
        "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
        "        )\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ug_Le7dk2cCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim=256, # from Table 1\n",
        "                     mlp_size=512, # from Table 1\n",
        "                     dropout=0.1) # from Table 3\n",
        "\n",
        "# Pass output of MSABlock through MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\n",
        "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
      ],
      "metadata": {
        "id": "Pr_AAjEs3vEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Transformer Encoder"
      ],
      "metadata": {
        "id": "ttRRyxlZ5Buh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Transformer Encoder by combining our custom made layers"
      ],
      "metadata": {
        "id": "PNrvKg1O5Z6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
        "                 mlp_size:int=512, # MLP size from Table 1 for ViT-Base\n",
        "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create MSA block (equation 2)\n",
        "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     attn_dropout=attn_dropout)\n",
        "\n",
        "        # 4. Create MLP block (equation 3)\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "\n",
        "    # 5. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 6. Create residual connection for MSA block (add the input to the output)\n",
        "        x =  self.msa_block(x) + x # Fix: Assign the result back to x\n",
        "\n",
        "        # 7. Create residual connection for MLP block (add the input to the output)\n",
        "        x = self.mlp_block(x) + x\n",
        "\n",
        "        return x # Return the output of the block"
      ],
      "metadata": {
        "id": "vrpdj5QZ4vno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of TransformerEncoderBlock\n",
        "transformer_encoder_block = TransformerEncoderBlock()\n",
        "\n",
        "# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
        "summary(model=transformer_encoder_block,\n",
        "         input_size=(1, 65, 256), # (batch_size, num_patches, embedding_dimension)\n",
        "         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "         col_width=20,\n",
        "         row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "1ORgIspX5iyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a Transformer Encoder with PyTorch's Transformer layers"
      ],
      "metadata": {
        "id": "fVC9rXq36BDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
        "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                                                             nhead=8, # Heads from Table 1 for ViT-Base\n",
        "                                                             dim_feedforward=512, # MLP size from Table 1 for ViT-Base\n",
        "                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                                                             activation=\"gelu\", # GELU non-linear activation\n",
        "                                                             batch_first=True, # Do our batches come first?\n",
        "                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n",
        "\n",
        "torch_transformer_encoder_layer"
      ],
      "metadata": {
        "id": "yqI1e7D95vu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n",
        "summary(model=torch_transformer_encoder_layer,\n",
        "        input_size=(1, 65, 256), # (batch_size, num_patches, embedding_dimension)\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "DfsBG3ue6IGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together to create ViT"
      ],
      "metadata": {
        "id": "qpaazQTK6XH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a ViT class that inherits from nn.Module\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 img_size:int=32, # Training resolution from Table 3 in ViT paper\n",
        "                 in_channels:int=3, # Number of channels in input image\n",
        "                 patch_size:int=4, # Patch size\n",
        "                 num_transformer_layers:int=8, # Layers from Table 1 for ViT-Base\n",
        "                 embedding_dim:int=256, # Hidden size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=512, # MLP size from Table 1 for ViT-Base\n",
        "                 num_heads:int=8, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0, # Dropout for attention projection\n",
        "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
        "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
        "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
        "        super().__init__() # don't forget the super().__init__()!\n",
        "\n",
        "        # 3. Make the image size is divisible by the patch size\n",
        "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "        # 4. Calculate number of patches (height * width/patch^2)\n",
        "        self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
        "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        # 6. Create learnable position embedding\n",
        "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "\n",
        "        # 7. Create embedding dropout value\n",
        "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "        # 8. Create patch embedding layer\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                              patch_size=patch_size,\n",
        "                                              embedding_dim=embedding_dim)\n",
        "\n",
        "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
        "        # Note: The \"*\" means \"all\"\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                            num_heads=num_heads,\n",
        "                                                                            mlp_size=mlp_size,\n",
        "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "        # 10. Create classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    # 11. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 12. Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
        "\n",
        "        # 14. Create patch embedding (equation 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concat class embedding and patch embedding (equation 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Add position embedding to patch embedding (equation 1)\n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Run embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 19. Put 0 index logit through classifier (equation 4)\n",
        "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "tZkxzrcp6LxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of creating the class embedding and expanding over a batch dimension\n",
        "batch_size = 128\n",
        "class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 256)) # create a single learnable class token\n",
        "class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n",
        "\n",
        "# Print out the change in shapes\n",
        "print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\n",
        "print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"
      ],
      "metadata": {
        "id": "Hj8JVVPw7D0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create a random tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(128, 3, 32, 32) # (batch_size, color_channels, height, width)\n",
        "\n",
        "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
        "vit = ViT(num_classes=len(class_names))\n",
        "\n",
        "# Pass the random image tensor to our ViT instance\n",
        "vit(random_image_tensor)"
      ],
      "metadata": {
        "id": "DJHmAqhM7I9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        " # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
        "summary(model=vit,\n",
        "        input_size=(128, 3, 32, 32), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "id": "QnW1g0SU7NuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up training code for our ViT model"
      ],
      "metadata": {
        "id": "8lZAt32O8ADz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Setup optimizer and loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    params=vit.parameters(),\n",
        "    lr=3e-4,\n",
        "    weight_decay=0.05\n",
        ")\n",
        "\n",
        "# Set the seeds\n",
        "set_seeds()\n",
        "\n",
        "# Train the model - pass OPTIMIZER not scheduler\n",
        "results = engine.train(\n",
        "    model=vit,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,  # ← Pass optimizer here\n",
        "    loss_fn=criterion,\n",
        "    epochs=40,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "lfJ-hHUz7nQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can compare the outcomes of each epoch in form of two graps in which first graph is Loss graph (which gives train loss v/s test loss) and second graph is Accuracy graph (which gives train acc v/s test acc)"
      ],
      "metadata": {
        "id": "cEhrBilgzuz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "# Plot our ViT model's loss curves\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "X_kS-O_jjdtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vyVRyHjsTm0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}