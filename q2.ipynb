{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text-Driven Image Segmentation with SAM 2\n",
        "\n",
        "## Project Overview\n",
        "This notebook demonstrates text-driven image segmentation using Meta's Segment Anything Model 2 (SAM 2). The pipeline converts natural language text prompts into precise object segmentation masks by combining text understanding models with SAM 2's powerful segmentation capabilities.\n",
        "\n",
        "## Pipeline Architecture\n",
        "1. **Text Understanding**: Convert text prompts to visual regions using GroundingDINO/CLIP-based models\n",
        "2. **Prompt Generation**: Transform text queries into spatial prompts (points/boxes)\n",
        "3. **Segmentation**: Feed prompts to SAM 2 to generate accurate object masks\n",
        "4. **Visualization**: Display results with mask overlays and confidence scores\n",
        "\n",
        "## Key Features\n",
        "- End-to-end text-to-mask segmentation\n",
        "- Support for various object categories\n",
        "- Real-time inference capabilities\n",
        "- Extensible to video segmentation (bonus)\n",
        "\n",
        "## Applications\n",
        "- Interactive image editing\n",
        "- Content-based image retrieval\n",
        "- Video object tracking\n",
        "- Automated image annotation"
      ],
      "metadata": {
        "id": "L1nK5iD0e7qP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell installs all necessary dependencies for the text-driven image segmentation pipeline:\n",
        "- PyTorch and Torchvision for deep learning framework\n",
        "- OpenCV and Matplotlib for image processing and visualization\n",
        "- SAM 2 (Segment Anything Model 2) from Facebook Research\n",
        "- Downloads sample images (truck.jpg and groceries.jpg) for testing\n",
        "- Downloads the pre-trained SAM 2 model checkpoint (sam2.1_hiera_large.pt)"
      ],
      "metadata": {
        "id": "NSGXpkQDfEL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
        "\n",
        "!mkdir -p images\n",
        "!wget -P images https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/truck.jpg\n",
        "!wget -P images https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/groceries.jpg\n",
        "\n",
        "!mkdir -p ../checkpoints/\n",
        "!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
      ],
      "metadata": {
        "id": "wCht8W1s7QCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports the core Python libraries required for the project:\n",
        "- OS for environment configuration\n",
        "- NumPy for numerical operations\n",
        "- PyTorch for tensor operations\n",
        "- Matplotlib for visualization\n",
        "- PIL for image handling\n",
        "- Configures MPS fallback for Apple Silicon compatibility"
      ],
      "metadata": {
        "id": "8_B2FLgxfJJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "evg4KC8iUUD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up the computation device for optimal performance:\n",
        "- Automatically detects and selects CUDA (NVIDIA GPU) if available\n",
        "- Falls back to MPS (Apple Silicon) or CPU as needed\n",
        "- Configures precision settings (bfloat16, tf32) for faster inference on compatible hardware\n",
        "- Provides warnings about preliminary MPS support"
      ],
      "metadata": {
        "id": "ngitas2hfMZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the device for computation\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    # use bfloat16 for the entire notebook\n",
        "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
        "    if torch.cuda.get_device_properties(0).major >= 8:\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "elif device.type == \"mps\":\n",
        "    print(\n",
        "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
        "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
        "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "tsoa8jBLVrjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines helper functions for displaying segmentation results:\n",
        "- `show_mask()`: Visualizes segmentation masks with optional random colors and borders\n",
        "- `show_points()`: Displays positive/negative point prompts on images\n",
        "- `show_box()`: Draws bounding boxes around detected objects\n",
        "- `show_masks()`: Comprehensive function to display masks with scores and input prompts"
      ],
      "metadata": {
        "id": "vDZl1mW7fvN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(3)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False, borders = True):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask = mask.astype(np.uint8)\n",
        "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    if borders:\n",
        "        import cv2\n",
        "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "        # Try to smooth contours\n",
        "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
        "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
        "\n",
        "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(image)\n",
        "        show_mask(mask, plt.gca(), borders=borders)\n",
        "        if point_coords is not None:\n",
        "            assert input_labels is not None\n",
        "            show_points(point_coords, input_labels, plt.gca())\n",
        "        if box_coords is not None:\n",
        "            # boxes\n",
        "            show_box(box_coords, plt.gca())\n",
        "        if len(scores) > 1:\n",
        "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "oD6AhD2qVvX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads and preprocesses the input image:\n",
        "- Opens the sample truck image using PIL\n",
        "- Converts the image to RGB numpy array format\n",
        "- Prepares the image for processing by SAM 2"
      ],
      "metadata": {
        "id": "Tq5QAhjrfz_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('images/truck.jpg')\n",
        "image = np.array(image.convert(\"RGB\"))"
      ],
      "metadata": {
        "id": "-47p2S6pVzKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell displays the original input image:\n",
        "- Creates a matplotlib figure to visualize the source image\n",
        "- Shows the image without any processing\n",
        "- Provides a baseline view before segmentation"
      ],
      "metadata": {
        "id": "dcKHJa_Af4Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "plt.axis('on')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yuMZ94TCV2-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting objects with SAM 2\n",
        "First, load the SAM 2 model and predictor. Change the path below to point to the SAM 2 checkpoint. Running on CUDA and using the default model are recommended for best results."
      ],
      "metadata": {
        "id": "kQdHvfh8f8O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
        "\n",
        "predictor = SAM2ImagePredictor(sam2_model)"
      ],
      "metadata": {
        "id": "kiK--5vPV5lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process the image to produce an image embedding by calling `SAM2ImagePredictor.set_image`. `SAM2ImagePredictor` remembers this embedding and will use it for subsequent mask prediction."
      ],
      "metadata": {
        "id": "orHyHqEaf9fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.set_image(image)"
      ],
      "metadata": {
        "id": "f1liXamZV9fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image."
      ],
      "metadata": {
        "id": "QK-wGwB_hOu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_point = np.array([[500, 375]])\n",
        "input_label = np.array([1])"
      ],
      "metadata": {
        "id": "YiiceLkaWDJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "show_points(input_point, input_label, plt.gca())\n",
        "plt.axis('on')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wkFAdp8EWQX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict with `SAM2ImagePredictor.predict`. The model returns masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction."
      ],
      "metadata": {
        "id": "dtv-5eFhhWYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictor._features[\"image_embed\"].shape, predictor._features[\"image_embed\"][-1].shape)"
      ],
      "metadata": {
        "id": "yIPZTtJWWTNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `multimask_output=True` (the default setting), SAM 2 outputs 3 masks, where `scores` gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When `False`, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in `scores`. This will often result in a better mask."
      ],
      "metadata": {
        "id": "W1qOQ8i7hloP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=True,\n",
        ")\n",
        "sorted_ind = np.argsort(scores)[::-1]\n",
        "masks = masks[sorted_ind]\n",
        "scores = scores[sorted_ind]\n",
        "logits = logits[sorted_ind]"
      ],
      "metadata": {
        "id": "iKdswzhfWZEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks.shape  # (number_of_masks) x H x W"
      ],
      "metadata": {
        "id": "IEXvKL5zWcex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)"
      ],
      "metadata": {
        "id": "mxXNA6iMWgGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specifying a specific object with additional points\n",
        "The single input point is ambiguous, and the model has returned multiple objects consistent with it. To obtain a single object, multiple points can be provided. If available, a mask from a previous iteration can also be supplied to the model to aid in prediction. When specifying a single object with multiple prompts, a single mask can be requested by setting `multimask_output=False`."
      ],
      "metadata": {
        "id": "OMGsBolGhzGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_point = np.array([[500, 375], [1125, 625]])\n",
        "input_label = np.array([1, 1])\n",
        "\n",
        "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask"
      ],
      "metadata": {
        "id": "TsZPpY5fWi39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, _ = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    mask_input=mask_input[None, :, :],\n",
        "    multimask_output=False,\n",
        ")"
      ],
      "metadata": {
        "id": "fzA0rlBeWqin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks.shape"
      ],
      "metadata": {
        "id": "ujX1BtxMW25k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"
      ],
      "metadata": {
        "id": "EdowsGGNW4wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_point = np.array([[500, 375], [1125, 625]])\n",
        "input_label = np.array([1, 0])\n",
        "\n",
        "mask_input = logits[np.argmax(scores), :, :]"
      ],
      "metadata": {
        "id": "ENAkcDucW8He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, _ = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    mask_input=mask_input[None, :, :],\n",
        "    multimask_output=False,\n",
        ")"
      ],
      "metadata": {
        "id": "ClHJ90NlXAt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"
      ],
      "metadata": {
        "id": "nI4eJV56XD9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specifying a specific object with a box\n",
        "The model can also take a box as input, provided in xyxy format."
      ],
      "metadata": {
        "id": "9xmeKdZqh90w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_box = np.array([425, 600, 700, 875])"
      ],
      "metadata": {
        "id": "ERQD43QkXGRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=input_box[None, :],\n",
        "    multimask_output=False,\n",
        ")"
      ],
      "metadata": {
        "id": "cZ0VgtkHXSgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_masks(image, masks, scores, box_coords=input_box)"
      ],
      "metadata": {
        "id": "b2w5C6zgXVXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining points and boxes\n",
        "Points and boxes may be combined, just by including both types of prompts to the predictor. Here this can be used to select just the trucks's tire, instead of the entire wheel."
      ],
      "metadata": {
        "id": "gtZKAMcTiGyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_box = np.array([425, 600, 700, 875])\n",
        "input_point = np.array([[575, 750]])\n",
        "input_label = np.array([0])"
      ],
      "metadata": {
        "id": "_X-D5m03XYRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, logits = predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    box=input_box,\n",
        "    multimask_output=False,\n",
        ")"
      ],
      "metadata": {
        "id": "OmOkp6rMXan3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_masks(image, masks, scores, box_coords=input_box, point_coords=input_point, input_labels=input_label)"
      ],
      "metadata": {
        "id": "i6yuBF9nXdwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batched prompt inputs\n",
        "`SAM2ImagePredictor` can take multiple input prompts for the same image, using `predict` method. For example, imagine we have several box outputs from an object detector."
      ],
      "metadata": {
        "id": "HFim5NruiPTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_boxes = np.array([\n",
        "    [75, 275, 1725, 850],\n",
        "    [425, 600, 700, 875],\n",
        "    [1375, 550, 1650, 800],\n",
        "    [1240, 675, 1400, 750],\n",
        "])"
      ],
      "metadata": {
        "id": "wuSNk5p-Xfhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks, scores, _ = predictor.predict(\n",
        "    point_coords=None,\n",
        "    point_labels=None,\n",
        "    box=input_boxes,\n",
        "    multimask_output=False,\n",
        ")"
      ],
      "metadata": {
        "id": "Kx4ZG3XyXin7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks.shape  # (batch_size) x (num_predicted_masks_per_input) x H x W"
      ],
      "metadata": {
        "id": "4MtHllHIXk8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "for mask in masks:\n",
        "    show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
        "for box in input_boxes:\n",
        "    show_box(box, plt.gca())\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MbGToyoPXo5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-end batched inference\n",
        "If all prompts are available in advance, it is possible to run SAM 2 directly in an end-to-end fashion. This also allows batching over images."
      ],
      "metadata": {
        "id": "UPg9Mn9HiY3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image1 = image  # truck.jpg from above\n",
        "image1_boxes = np.array([\n",
        "    [75, 275, 1725, 850],\n",
        "    [425, 600, 700, 875],\n",
        "    [1375, 550, 1650, 800],\n",
        "    [1240, 675, 1400, 750],\n",
        "])\n",
        "\n",
        "image2 = Image.open('images/groceries.jpg')\n",
        "image2 = np.array(image2.convert(\"RGB\"))\n",
        "image2_boxes = np.array([\n",
        "    [450, 170, 520, 350],\n",
        "    [350, 190, 450, 350],\n",
        "    [500, 170, 580, 350],\n",
        "    [580, 170, 640, 350],\n",
        "])\n",
        "\n",
        "img_batch = [image1, image2]\n",
        "boxes_batch = [image1_boxes, image2_boxes]"
      ],
      "metadata": {
        "id": "rhA9hdTSXrW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.set_image_batch(img_batch)"
      ],
      "metadata": {
        "id": "_lSbiEWvYCq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks_batch, scores_batch, _ = predictor.predict_batch(\n",
        "    None,\n",
        "    None,\n",
        "    box_batch=boxes_batch,\n",
        "    multimask_output=False\n",
        ")"
      ],
      "metadata": {
        "id": "NaZr8-GtYE4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, boxes, masks in zip(img_batch, boxes_batch, masks_batch):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image)\n",
        "    for mask in masks:\n",
        "        show_mask(mask.squeeze(0), plt.gca(), random_color=True)\n",
        "    for box in boxes:\n",
        "        show_box(box, plt.gca())"
      ],
      "metadata": {
        "id": "S6wNBeY3YG1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus Section-Video segmentation**"
      ],
      "metadata": {
        "id": "Ny_WBtIJYPE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It covers following:\n",
        "- adding clicks (or box) on a frame to get and refine masklets (spatio-temporal masks)\n",
        "- propagating clicks (or box) to get masklets throughout the video\n",
        "- segmenting and tracking multiple objects at the same time\n",
        "We use the terms segment or mask to refer to the model prediction for an object on a single frame, and masklet to refer to the spatio-temporal masks across the entire video."
      ],
      "metadata": {
        "id": "q-038iFRjENt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment Set-up**"
      ],
      "metadata": {
        "id": "7PD19cGqjdYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p videos\n",
        "!wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
        "!unzip -d videos videos/bedroom.zip\n",
        "\n",
        "!mkdir -p ../checkpoints/\n",
        "!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
      ],
      "metadata": {
        "id": "rCdcXEsQYJDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the SAM 2 video predictor**"
      ],
      "metadata": {
        "id": "yIvpMWRJjtAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
        "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
      ],
      "metadata": {
        "id": "uUYh0-k2YtDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines helper functions for displaying segmentation results:\n",
        "- `show_mask()`: Visualizes segmentation masks with optional random colors and borders\n",
        "- `show_points()`: Displays positive/negative point prompts on images\n",
        "- `show_box()`: Draws bounding boxes around detected objects\n",
        "- `show_masks()`: Comprehensive function to display masks with scores and input prompts"
      ],
      "metadata": {
        "id": "2M84H9ANj4f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask_v(mask, ax, obj_id=None, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        cmap = plt.get_cmap(\"tab10\")\n",
        "        cmap_idx = 0 if obj_id is None else obj_id\n",
        "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "\n",
        "def show_points_v(coords, labels, ax, marker_size=200):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "\n",
        "def show_box_v(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
      ],
      "metadata": {
        "id": "UpTwCqeVZHcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select an example video**\n",
        "\n",
        "We assume that the video is stored as a list of JPEG frames with filenames like <frame_index>.jpg."
      ],
      "metadata": {
        "id": "iMdEQJIWkAt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
        "video_dir = \"./videos/bedroom\"\n",
        "\n",
        "# scan all the JPEG frame names in this directory\n",
        "frame_names = [\n",
        "    p for p in os.listdir(video_dir)\n",
        "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
        "]\n",
        "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
        "\n",
        "# take a look the first video frame\n",
        "frame_idx = 0\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
      ],
      "metadata": {
        "id": "SKCeXIVjZWks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the inference state**\n",
        "\n",
        "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an inference state on this video.\n",
        "\n",
        "During initialization, it loads all the JPEG frames in video_path and stores their pixels in inference_state (as shown in the progress bar below)."
      ],
      "metadata": {
        "id": "0bLm_d9bkIPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inference_state = predictor.init_state(video_path=video_dir)"
      ],
      "metadata": {
        "id": "EZb019NUZfIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Segment & track one object**"
      ],
      "metadata": {
        "id": "dTvr6MGwkODX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.reset_state(inference_state)"
      ],
      "metadata": {
        "id": "7M0PtKv4ZluS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_frame_idx = 0  # the frame index we interact with\n",
        "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
        "\n",
        "# Let's add a 2nd positive click at (x, y) = (250, 220) to refine the mask\n",
        "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
        "points = np.array([[210, 350], [250, 220]], dtype=np.float32)\n",
        "# for labels, `1` means positive click and `0` means negative click\n",
        "labels = np.array([1, 1], np.int32)\n",
        "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "    inference_state=inference_state,\n",
        "    frame_idx=ann_frame_idx,\n",
        "    obj_id=ann_obj_id,\n",
        "    points=points,\n",
        "    labels=labels,\n",
        ")\n",
        "\n",
        "# show the results on the current (interacted) frame\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {ann_frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
        "show_points_v(points, labels, plt.gca())\n",
        "show_mask_v((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
      ],
      "metadata": {
        "id": "zT5H8OSjZxVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Propagate the prompts to get the masklet across the video**"
      ],
      "metadata": {
        "id": "4sDH0D0mkgHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run propagation throughout the video and collect the results in a dict\n",
        "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "\n",
        "# render the segmentation results every few frames\n",
        "vis_frame_stride = 30\n",
        "plt.close(\"all\")\n",
        "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.title(f\"frame {out_frame_idx}\")\n",
        "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
        "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
        "        show_mask_v(out_mask, plt.gca(), obj_id=out_obj_id)"
      ],
      "metadata": {
        "id": "-H5J7_MfZyCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Segment an object using box prompt**"
      ],
      "metadata": {
        "id": "jucZTh-ikp4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.reset_state(inference_state)"
      ],
      "metadata": {
        "id": "zJqRYQMZaQeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_frame_idx = 0  # the frame index we interact with\n",
        "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
        "\n",
        "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
        "box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
        "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "    inference_state=inference_state,\n",
        "    frame_idx=ann_frame_idx,\n",
        "    obj_id=ann_obj_id,\n",
        "    box=box,\n",
        ")\n",
        "\n",
        "# show the results on the current (interacted) frame\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {ann_frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
        "show_box_v(box, plt.gca())\n",
        "show_mask_v((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
      ],
      "metadata": {
        "id": "ha8Q6nZUbMrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ann_frame_idx = 0  # the frame index we interact with\n",
        "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
        "\n",
        "# Let's add a positive click at (x, y) = (460, 60) to refine the mask\n",
        "points = np.array([[460, 60]], dtype=np.float32)\n",
        "# for labels, `1` means positive click and `0` means negative click\n",
        "labels = np.array([1], np.int32)\n",
        "# note that we also need to send the original box input along with\n",
        "# the new refinement click together into `add_new_points_or_box`\n",
        "box = np.array([300, 0, 500, 400], dtype=np.float32)\n",
        "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
        "    inference_state=inference_state,\n",
        "    frame_idx=ann_frame_idx,\n",
        "    obj_id=ann_obj_id,\n",
        "    points=points,\n",
        "    labels=labels,\n",
        "    box=box,\n",
        ")\n",
        "\n",
        "# show the results on the current (interacted) frame\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.title(f\"frame {ann_frame_idx}\")\n",
        "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
        "show_box(box, plt.gca())\n",
        "show_points_v(points, labels, plt.gca())\n",
        "show_mask_v((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
      ],
      "metadata": {
        "id": "7SLg3M93bO9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run propagation throughout the video and collect the results in a dict\n",
        "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
        "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
        "    video_segments[out_frame_idx] = {\n",
        "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "        for i, out_obj_id in enumerate(out_obj_ids)\n",
        "    }\n",
        "\n",
        "# render the segmentation results every few frames\n",
        "vis_frame_stride = 30\n",
        "plt.close(\"all\")\n",
        "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.title(f\"frame {out_frame_idx}\")\n",
        "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
        "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
        "        show_mask_v(out_mask, plt.gca(), obj_id=out_obj_id)"
      ],
      "metadata": {
        "id": "bbImDdQ9bWu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6P17E37bbTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}